{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"![MLCats](https://res.cloudinary.com/practicaldev/image/fetch/s--trGSZ1sP--/c_imagga_scale,f_auto,fl_progressive,h_420,q_auto,w_1000/https://dev-to-uploads.s3.amazonaws.com/i/dd9sgp1plounbt06h904.jpg)","metadata":{}},{"cell_type":"code","source":"import os\n\nimport math, random\nimport numpy as np\nimport matplotlib.pyplot as plt\nplt.rc('figure', figsize=(20, 5))\n\nimport torch\nimport torchaudio\nfrom torchaudio import transforms\n\nimport librosa\nimport librosa.display\n\nfrom IPython.display import Audio","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-11-24T16:16:25.841205Z","iopub.execute_input":"2021-11-24T16:16:25.841556Z","iopub.status.idle":"2021-11-24T16:16:29.423717Z","shell.execute_reply.started":"2021-11-24T16:16:25.841473Z","shell.execute_reply":"2021-11-24T16:16:29.422592Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Lets get to know the data better","metadata":{}},{"cell_type":"code","source":"DATA_DIR = '../input/cat-meow-classification/dataset/dataset/'\naudio_files = os.listdir(DATA_DIR)","metadata":{"execution":{"iopub.status.busy":"2021-11-24T16:16:29.425572Z","iopub.execute_input":"2021-11-24T16:16:29.425838Z","iopub.status.idle":"2021-11-24T16:16:29.478943Z","shell.execute_reply.started":"2021-11-24T16:16:29.425802Z","shell.execute_reply":"2021-11-24T16:16:29.478019Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Summary Statistics\n\ncategory_count = {'F':0,'I':0,'B':0,'MC':0,'EU':0,'MN':0,'FN':0}\n\nfor file in audio_files:\n    decoded = file.split('_')\n    for element in decoded:\n        if element in category_count.keys():\n            category_count[element] +=1\nprint(category_count)\n\nlabel = {\n    'Waiting For Food':category_count['F'],\n    'Isolated in unfamiliar Environment':category_count['I'],\n    'Brushing':category_count['B'],\n}\nbreed = {\n    'Maine Coon':category_count['MC'],\n    'European Shorthair':category_count['EU'],\n}\ngender = {\n    'Male':category_count['MN'],\n    'Female':category_count['FN'],\n}\nplt.bar(list(label.keys()),list(label.values()))\nplt.bar(list(breed.keys()),list(breed.values()))\nplt.bar(list(gender.keys()),list(gender.values()))","metadata":{"execution":{"iopub.status.busy":"2021-11-24T16:16:29.480452Z","iopub.execute_input":"2021-11-24T16:16:29.480695Z","iopub.status.idle":"2021-11-24T16:16:29.908541Z","shell.execute_reply.started":"2021-11-24T16:16:29.480666Z","shell.execute_reply":"2021-11-24T16:16:29.907995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let us now find out about the lengths of the provided audio samples","metadata":{}},{"cell_type":"code","source":"audio_lengths = []\n\nfor file in audio_files:\n    audio_lengths.append(librosa.get_duration(filename=os.path.join(DATA_DIR,file)))\n\nsample_num = np.arange(len(audio_lengths))\n\nplt.bar(sample_num, audio_lengths)\nplt.xlabel(\"Audio Sample\")\nplt.ylabel(\"Seconds\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-24T16:16:29.910071Z","iopub.execute_input":"2021-11-24T16:16:29.9105Z","iopub.status.idle":"2021-11-24T16:16:32.802212Z","shell.execute_reply.started":"2021-11-24T16:16:29.910467Z","shell.execute_reply":"2021-11-24T16:16:32.801256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Seeing Sounds!\nsounds strange !?!?","metadata":{}},{"cell_type":"code","source":"sample_audio = '../input/cat-meow-classification/dataset/dataset/B_ANI01_MC_FN_SIM01_101.wav'","metadata":{"execution":{"iopub.status.busy":"2021-11-24T16:16:32.803462Z","iopub.execute_input":"2021-11-24T16:16:32.803941Z","iopub.status.idle":"2021-11-24T16:16:32.807564Z","shell.execute_reply.started":"2021-11-24T16:16:32.803907Z","shell.execute_reply":"2021-11-24T16:16:32.806528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"samples, sample_rate = librosa.load(sample_audio)\nlibrosa.display.waveplot(samples, sr=sample_rate)","metadata":{"execution":{"iopub.status.busy":"2021-11-24T16:16:32.80924Z","iopub.execute_input":"2021-11-24T16:16:32.809458Z","iopub.status.idle":"2021-11-24T16:16:33.960943Z","shell.execute_reply.started":"2021-11-24T16:16:32.809431Z","shell.execute_reply":"2021-11-24T16:16:33.960041Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Spectrogram\n\nWhen dealing with Audio data, Deep Learning models tend not take to directly take the waveform as the input but as a spectrogram. \n\n**What is a spectrogram?**\n> A spectrogram is a visual way of representing a signals's strength or loudness over time at various frequencies present in a particular waveform. Not only can one see wheather there is more less energy at two particular frequencies, but one can also see how energy levels vary over time.\n\nSpectrograms are generated from sound signals using Fourier Transforms. A Fourier Transform decomposes the signal into its constituent frequencies and displays the amplitude of each frequency present in the signal.\n\nA Spectrogram chops up the duration of the sound signal into smaller time segments and then applies the Fourier Transform to each segment, to determine the frequencies contained in that segment. It then combines the Fourier Transforms for all those segments into a single plot.\n\nIt plots Frequency (y-axis) vs Time (x-axis) and uses different colors to indicate the Amplitude of each frequency. The brighter the color the higher the energy of the signal.\n\nA spectrogram now boils down to an image. Now that we have a picture representing a sound we can train all sorts of vision based models to deal with the given audio.","metadata":{}},{"cell_type":"markdown","source":"## Mel Spectrogram\nHumans percieve frequencies on a logarithmic scale rather than a linear scale. To account for this difference, the Mel Scale was developed.\n> The mel scale is a perceptual scale of pitches judged by listeners to be equal in distance from one another. The reference point between this scale and normal frequency measurement is defined by assigning a perceptual pitch of 1000 mels to a 1000 Hz tone, 40 dB above the listener's threshold. Above about 500 Hz, increasingly large intervals are judged by listeners to produce equal pitch increments.\n\n![Mel Scale](https://www.sfu.ca/sonic-studio-webdav/handbook/Graphics/Mel.gif)\n\nA Mel spectrogram differs from a regular spectrogram by\n1. It uses the Mel Scale instead of Frequency\n2. It uses Decibel scale instead of amplitude to indicate colour","metadata":{}},{"cell_type":"code","source":"sgram = librosa.stft(samples)\nsgram_mag, _ = librosa.magphase(sgram)\nmel_scale_sgram = librosa.feature.melspectrogram(S=sgram_mag, sr=sample_rate,n_mels=512)\nmel_sgram = librosa.amplitude_to_db(mel_scale_sgram, ref=np.min)\nlibrosa.display.specshow(mel_sgram, sr=sample_rate, x_axis='time', y_axis='mel')\nplt.colorbar(format='%+2.0f dB')","metadata":{"execution":{"iopub.status.busy":"2021-11-24T16:16:33.962075Z","iopub.execute_input":"2021-11-24T16:16:33.96231Z","iopub.status.idle":"2021-11-24T16:16:34.451138Z","shell.execute_reply.started":"2021-11-24T16:16:33.962282Z","shell.execute_reply":"2021-11-24T16:16:34.450255Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Resources\n1. [Audio Deep Learning Made Simple](https://towardsdatascience.com/audio-deep-learning-made-simple-part-2-why-mel-spectrograms-perform-better-aad889a93505)\n\n2. [librosa docs](https://librosa.org/doc/latest/index.html)\n","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}